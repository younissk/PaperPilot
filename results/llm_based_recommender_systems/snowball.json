{
  "query": "LLM Based Recommender systems",
  "total_accepted": 67,
  "papers": [
    {
      "paper_id": "W4403851269",
      "title": "A Prompting-Based Representation Learning Method for Recommendation with Large Language Models",
      "year": 2024,
      "citation_count": 0,
      "discovered_from": "evaluation metrics for LLM based recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "In recent years, Recommender Systems (RS) have witnessed a transformative shift with the advent of Large Language Models (LLMs) in the field of Natural Language Processing (NLP). Models such as GPT-3.5/4, Llama, have demonstrated unprecedented capabilities in understanding and generating human-like text. The extensive information pre-trained by these LLMs allows for the potential to capture a more profound semantic representation from different contextual information of users and items.   While "
    },
    {
      "paper_id": "W7078700161",
      "title": "Revealing Potential Biases in LLM-Based Recommender Systems in the Cold Start Setting",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "evaluation metrics for LLM based recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Large Language Models (LLMs) are increasingly used for recommendation tasks due to their general-purpose capabilities. While LLMs perform well in rich-context settings, their behavior in cold-start scenarios, where only limited signals such as age, gender, or language are available, raises fairness concerns because they may rely on societal biases encoded during pretraining. We introduce a benchmark specifically designed to evaluate fairness in zero-context recommendation. Our modular pipeline s"
    },
    {
      "paper_id": "W4404343192",
      "title": "ReasoningRec: Bridging Personalized Recommendations and Human-Interpretable Explanations through LLM Reasoning",
      "year": 2024,
      "citation_count": 0,
      "discovered_from": "evaluation metrics for LLM based recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "This paper presents ReasoningRec, a reasoning-based recommendation framework that leverages Large Language Models (LLMs) to bridge the gap between recommendations and human-interpretable explanations. In contrast to conventional recommendation systems that rely on implicit user-item interactions, ReasoningRec employs LLMs to model users and items, focusing on preferences, aversions, and explanatory reasoning. The framework utilizes a larger LLM to generate synthetic explanations for user prefere"
    },
    {
      "paper_id": "W6967265803",
      "title": "Behavior Alignment: A New Perspective of Evaluating LLM-based Conversational Recommender Systems",
      "year": 2024,
      "citation_count": 0,
      "discovered_from": "evaluation metrics for LLM based recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Large Language Models (LLMs) have demonstrated great potential in Conversational Recommender Systems (CRS). However, the application of LLMs to CRS has exposed a notable discrepancy in behavior between LLM-based CRS and human recommenders: LLMs often appear inflexible and passive, frequently rushing to complete the recommendation task without sufficient inquiry.This behavior discrepancy can lead to decreased accuracy in recommendations and lower user satisfaction. Despite its importance, existin"
    },
    {
      "paper_id": "W6891779303",
      "title": "FairEval: Evaluating Fairness in LLM-Based Recommendations with Personality Awareness",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "evaluation metrics for LLM based recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Recent advances in Large Language Models (LLMs) have enabled their application to recommender systems (RecLLMs), yet concerns remain regarding fairness across demographic and psychological user dimensions. We introduce FairEval, a novel evaluation framework to systematically assess fairness in LLM-based recommendations. FairEval integrates personality traits with eight sensitive demographic attributes,including gender, race, and age, enabling a comprehensive assessment of user-level bias. We eva"
    },
    {
      "paper_id": "W4415935814",
      "title": "Effectiveness of LLMs in Temporal User Profiling for Recommendation",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "evaluation metrics for LLM based recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Effectively modeling the dynamic nature of user preferences is crucial for enhancing recommendation accuracy and fostering transparency in recommender systems. Traditional user profiling often overlooks the distinction between transitory short-term interests and stable long-term preferences. This paper examines the capability of leveraging Large Language Models (LLMs) to capture these temporal dynamics, generating richer user representations through distinct short-term and long-term textual summ"
    },
    {
      "paper_id": "W4414879540",
      "title": "Exploring the Potential of LLMs for Serendipity Evaluation in Recommender Systems",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "evaluation metrics for LLM based recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Serendipity plays a pivotal role in enhancing user satisfaction within recommender systems, yet its evaluation poses significant challenges due to its inherently subjective nature and conceptual ambiguity. Current algorithmic approaches predominantly rely on proxy metrics for indirect assessment, often failing to align with real user perceptions, thus creating a gap. With large language models (LLMs) increasingly revolutionizing evaluation methodologies across various human annotation tasks, we "
    },
    {
      "paper_id": "W4310514063",
      "title": "Outfit Generation and Recommendation -- An Experimental Study",
      "year": 2022,
      "citation_count": 0,
      "discovered_from": "LLM Based Recommender systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Over the past years, fashion-related challenges have gained a lot of attention in the research community. Outfit generation and recommendation, i.e., the composition of a set of items of different types (e.g., tops, bottom, shoes, accessories) that go well together, are among the most challenging ones. That is because items have to be both compatible amongst each other and also personalized to match the taste of the customer. Recently there has been a plethora of work targeted at tackling these "
    },
    {
      "paper_id": "W6929268638",
      "title": "M2TRec: Metadata-aware Multi-task Transformer for Large-scale and Cold-start free Session-based Recommendations",
      "year": 2022,
      "citation_count": 0,
      "discovered_from": "LLM Based Recommender systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Session-based recommender systems (SBRSs) have shown superior performance over conventional methods. However, they show limited scalability on large-scale industrial datasets since most models learn one embedding per item. This leads to a large memory requirement (of storing one vector per item) and poor performance on sparse sessions with cold-start or unpopular items. Using one public and one large industrial dataset, we experimentally show that state-of-the-art SBRSs have low performance on s"
    },
    {
      "paper_id": "W6967097542",
      "title": "Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "LLM Based Recommender systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Large Language Models (LLMs) are increasingly being implemented as joint decision-makers and explanation generators for Group Recommender Systems (GRS). In this paper, we evaluate these recommendations and explanations by comparing them to social choice-based aggregation strategies. Our results indicate that LLM-generated recommendations often resembled those produced by Additive Utilitarian (ADD) aggregation. However, the explanations typically referred to averaging ratings (resembling but not "
    },
    {
      "paper_id": "W4415102543",
      "title": "Towards Next-Generation Recommender Systems: A Benchmark for Personalized Recommendation Assistant with LLMs",
      "year": 2025,
      "citation_count": 1,
      "discovered_from": "LLM algorithms for personalized recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Recommender systems (RecSys) are widely used across various modern digital platforms and have garnered significant attention. Traditional recommender systems usually focus only on fixed and simple recommendation scenarios, making it difficult to generalize to new and unseen recommendation tasks in an interactive paradigm. Recently, the advancement of large language models (LLMs) has revolutionized the foundational architecture of RecSys, driving their evolution into more intelligent and interact"
    },
    {
      "paper_id": "W4401978522",
      "title": "Leveraging LLM Reasoning Enhances Personalized Recommender Systems",
      "year": 2024,
      "citation_count": 0,
      "discovered_from": "LLM algorithms for personalized recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Recent advancements have showcased the potential of Large Language Models (LLMs) in executing reasoning tasks, particularly facilitated by Chain-of-Thought (CoT) prompting. While tasks like arithmetic reasoning involve clear, definitive answers and logical chains of thought, the application of LLM reasoning in recommendation systems (RecSys) presents a distinct challenge. RecSys tasks revolve around subjectivity and personalized preferences, an under-explored domain in utilizing LLMs' reasoning "
    },
    {
      "paper_id": "W4416011514",
      "title": "Memory Assisted LLM for Personalized Recommendation System",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "LLM algorithms for personalized recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Large language models (LLMs) have demonstrated significant potential in solving recommendation tasks. With proven capabilities in understanding user preferences, LLM personalization has emerged as a critical area for providing tailored responses to individuals. Current studies explore personalization through prompt design and fine-tuning, paving the way for further research in personalized LLMs. However, existing approaches are either costly and inefficient in capturing diverse user preferences "
    },
    {
      "paper_id": "W4385681734",
      "title": "Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM",
      "year": 2023,
      "citation_count": 1,
      "discovered_from": "LLM algorithms for personalized recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "The analysis and mining of user heterogeneous behavior are of paramount importance in recommendation systems. However, the conventional approach of incorporating various types of heterogeneous behavior into recommendation models leads to feature sparsity and knowledge fragmentation issues. To address this challenge, we propose a novel approach for personalized recommendation via Large Language Model (LLM), by extracting and fusing heterogeneous knowledge from user heterogeneous behavior informat"
    },
    {
      "paper_id": "W4405354727",
      "title": "MOPI-HFRS: A Multi-objective Personalized Health-aware Food Recommendation System with LLM-enhanced Interpretation",
      "year": 2024,
      "citation_count": 4,
      "discovered_from": "LLM algorithms for personalized recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "The prevalence of unhealthy eating habits has become an increasingly concerning issue in the United States. However, major food recommendation platforms (e.g., Yelp) continue to prioritize users' dietary preferences over the healthiness of their choices. Although efforts have been made to develop health-aware food recommendation systems, the personalization of such systems based on users' specific health conditions remains under-explored. In addition, few research focus on the interpretability o"
    },
    {
      "paper_id": "W4416284157",
      "title": "A Language-Driven Framework for Improving Personalized Recommendations: Merging LLMs with Traditional Algorithms",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "LLM algorithms for personalized recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Traditional recommendation algorithms are not designed to provide personalized recommendations based on user preferences provided through text, e.g., \"I enjoy light-hearted comedies with a lot of humor\". Large Language Models (LLMs) have emerged as one of the most promising tools for natural language processing in recent years. This research proposes a novel framework that mimics how a close friend would recommend items based on their knowledge of an individual's tastes. We leverage LLMs to enha"
    },
    {
      "paper_id": "W4414991285",
      "title": "Research on Conversational Recommender System Considering Consumer Types",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "LLM algorithms for personalized recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Conversational Recommender Systems (CRS) provide personalized services through multi-turn interactions, yet most existing methods overlook users' heterogeneous decision-making styles and knowledge levels, which constrains both accuracy and efficiency. To address this gap, we propose CT-CRS (Consumer Type-Enhanced Conversational Recommender System), a framework that integrates consumer type modeling into dialogue recommendation. Based on consumer type theory, we define four user categories--depen"
    },
    {
      "paper_id": "W7092283700",
      "title": "MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "LLM algorithms for personalized recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "The application of Large Language Models (LLMs) in recommender systems faces key challenges in delivering deep personalization and intelligent reasoning, especially for interactive scenarios. Current methods are often constrained by limited context windows and single-turn reasoning, hindering their ability to capture dynamic user preferences and proactively reason over recommendation contexts. To address these limitations, we propose MR.Rec, a novel framework that synergizes memory and reasoning"
    },
    {
      "paper_id": "W6929462687",
      "title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "LLM algorithms for personalized recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "The emergence of agentic recommender systems powered by Large Language Models (LLMs) represents a paradigm shift in personalized recommendations, leveraging LLMs' advanced reasoning and role-playing capabilities to enable autonomous, adaptive decision-making. Unlike traditional recommendation approaches, agentic recommender systems can dynamically gather and interpret user-item interactions from complex environments, generating robust recommendation strategies that generalize across diverse scen"
    },
    {
      "paper_id": "W7083676386",
      "title": "SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "LLM algorithms for personalized recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "The dominant retrieve-then-rank pipeline in large-scale recommender systems suffers from mis-calibration and engineering overhead due to its architectural split and differing optimization objectives. While recent generative sequence models have shown promise in unifying retrieval and ranking by auto-regressively generating ranked items, existing solutions typically address either personalized search or query-free recommendation, often exhibiting performance trade-offs when attempting to unify bo"
    },
    {
      "paper_id": "W4386148472",
      "title": "LKPNR: LLM and KG for Personalized News Recommendation Framework",
      "year": 2023,
      "citation_count": 7,
      "discovered_from": "LLM algorithms for personalized recommendation systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Accurately recommending candidate news articles to users is a basic challenge faced by personalized news recommendation systems. Traditional methods are usually difficult to grasp the complex semantic information in news texts, resulting in unsatisfactory recommendation results. Besides, these traditional methods are more friendly to active users with rich historical behaviors. However, they can not effectively solve the \"long tail problem\" of inactive users. To address these issues, this resear"
    },
    {
      "paper_id": "W7106248926",
      "title": "Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "limitations and challenges in LLM recommender systems",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ra"
    },
    {
      "paper_id": "W6891788864",
      "title": "RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "user feedback integration in LLM based recommendation models",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Conversational recommender systems (CRS) based on Large Language Models (LLMs) need to constantly be aligned to the user preferences to provide satisfying and context-relevant item recommendations. The traditional supervised fine-tuning cannot capture the implicit feedback signal, e.g., dwell time, sentiment polarity, or engagement patterns. In this paper, we share a fine-tuning solution using human feedback reinforcement learning (RLHF) to maximize implied user feedback (IUF) in a multi-turn re"
    },
    {
      "paper_id": "W4416119106",
      "title": "User Feedback Alignment for LLM-powered Exploration in Large-scale Recommendation Systems",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "user feedback integration in LLM based recommendation models",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Exploration, the act of broadening user experiences beyond their established preferences, is challenging in large-scale recommendation systems due to feedback loops and limited signals on user exploration patterns. Large Language Models (LLMs) offer potential solutions by leveraging their world knowledge to recommend novel content outside these loops. A key challenge is aligning LLMs with user preferences while preserving their knowledge and reasoning. To enhance planning for new user interests "
    },
    {
      "paper_id": "W4414691664",
      "title": "What Matters in LLM-Based Feature Extractor for Recommender? A Systematic Analysis of Prompts, Models, and Adaptation",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "user feedback integration in LLM based recommendation models",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Using Large Language Models (LLMs) to generate semantic features has been demonstrated as a powerful paradigm for enhancing Sequential Recommender Systems (SRS). This typically involves three stages: processing item text, extracting features with LLMs, and adapting them for downstream models. However, existing methods vary widely in prompting, architecture, and adaptation strategies, making it difficult to fairly compare design choices and identify what truly drives performance. In this work, we"
    },
    {
      "paper_id": "W4398795244",
      "title": "Lusifer: LLM-based User SImulated Feedback Environment for online Recommender systems",
      "year": 2024,
      "citation_count": 1,
      "discovered_from": "user feedback integration in LLM based recommendation models",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Reinforcement learning (RL) recommender systems often rely on static datasets that fail to capture the fluid, ever changing nature of user preferences in real-world scenarios. Meanwhile, generative AI techniques have emerged as powerful tools for creating synthetic data, including user profiles and behaviors. Recognizing this potential, we introduce Lusifer, an LLM-based simulation environment designed to generate dynamic, realistic user feedback for RL-based recommender training. In Lusifer, us"
    },
    {
      "paper_id": "W4404313621",
      "title": "Agentic Feedback Loop Modeling Improves Recommendation and User Simulation",
      "year": 2024,
      "citation_count": 0,
      "discovered_from": "user feedback integration in LLM based recommendation models",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Large language model-based agents are increasingly applied in the recommendation field due to their extensive knowledge and strong planning capabilities. While prior research has primarily focused on enhancing either the recommendation agent or the user agent individually, the collaborative interaction between the two has often been overlooked. Towards this research gap, we propose a novel framework that emphasizes the feedback loop process to facilitate the collaboration between the recommendat"
    },
    {
      "paper_id": "W4415347986",
      "title": "Heterogeneous User Modeling for LLM-based Recommendation",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "user feedback integration in LLM based recommendation models",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "Leveraging Large Language Models (LLMs) for recommendation has demonstrated notable success in various domains, showcasing their potential for open-domain recommendation. A key challenge to advancing open-domain recommendation lies in effectively modeling user preferences from users' heterogeneous behaviors across multiple domains. Existing approaches, including ID-based and semantic-based modeling, struggle with poor generalization, an inability to compress noisy interactions effectively, and t"
    },
    {
      "paper_id": "W4407806925",
      "title": "LLM-based User Profile Management for Recommender System",
      "year": 2025,
      "citation_count": 0,
      "discovered_from": "user feedback integration in LLM based recommendation models",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evo"
    },
    {
      "paper_id": "W4407632341",
      "title": "A Survey on LLM-based News Recommender Systems",
      "year": 2025,
      "citation_count": 1,
      "discovered_from": "survey of LLM based recommender systems and applications",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0,
      "abstract": "News recommender systems play a critical role in mitigating the information overload problem. In recent years, due to the successful applications of large language model technologies, researchers have utilized Discriminative Large Language Models (DLLMs) or Generative Large Language Models (GLLMs) to improve the performance of news recommender systems. Although several recent surveys review significant challenges for deep learning-based news recommender systems, such as fairness, privacy-preserv"
    },
    {
      "paper_id": "S2:2e92b3699668f920a8d692535622ebeaa53315e2",
      "title": "Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation",
      "year": 2023,
      "citation_count": 221,
      "discovered_from": "W6891779303",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Addresses LLMs in recommendation with a focus on fairness.",
      "judge_confidence": 0.9,
      "abstract": "The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm — Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the diffe"
    },
    {
      "paper_id": "S2:a35f1315e91513ff0bec0c488fe175214fd9636c",
      "title": "Recommender Systems in the Era of Large Language Models (LLMs)",
      "year": 2023,
      "citation_count": 439,
      "discovered_from": "W4403851269",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly reviews LLM applications in recommender systems.",
      "judge_confidence": 0.9,
      "abstract": "With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an indispensable and important component, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have achieved significant advancements in enhancing recommender systems, these DNN-based methods still exhibit some limitations, such as inferior capabilities to effectively capture textual side information about users and items, difficulties in generalization"
    },
    {
      "paper_id": "S2:91d70c7f8c114d3b9056835316ffaee527fc4f53",
      "title": "Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System",
      "year": 2024,
      "citation_count": 94,
      "discovered_from": "W4407806925",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in collaborative filtering for recommendations.",
      "judge_confidence": 0.9,
      "abstract": "Collaborative filtering recommender systems (CF-RecSys) have shown successive results in enhancing the user experience on social media and e-commerce platforms. However, as CF-RecSys struggles under cold scenarios with sparse user-item interactions, recent strategies have focused on leveraging modality information of user/items (e.g., text or images) based on pre-trained modality encoders and Large Language Models (LLMs). Despite their effectiveness under cold scenarios, we observe that they und"
    },
    {
      "paper_id": "S2:5aa3b1009955ce2c8f896e0d5e94e06155ef1e43",
      "title": "LLMRec: Large Language Models with Graph Augmentation for Recommendation",
      "year": 2023,
      "citation_count": 330,
      "discovered_from": "W4407806925",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in recommendation systems with novel augmentation strategies.",
      "judge_confidence": 0.9,
      "abstract": "The problem of data sparsity has long been a challenge in recommendation systems, and previous studies have attempted to address this issue by incorporating side information. However, this approach often introduces side effects such as noise, availability issues, and low data quality, which in turn hinder the accurate modeling of user preferences and adversely impact recommendation performance. In light of the recent advancements in large language models (LLMs), which possess extensive knowledge"
    },
    {
      "paper_id": "S2:3487c12512fa41d3a4d64f00cb842525a8590ad3",
      "title": "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation",
      "year": 2023,
      "citation_count": 580,
      "discovered_from": "W4403851269",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs for recommendation systems.",
      "judge_confidence": 0.9,
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disp"
    },
    {
      "paper_id": "S2:f4e723958a93762befb4d4a039b44a7d752f9917",
      "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems",
      "year": 2023,
      "citation_count": 461,
      "discovered_from": "W4386148472",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs for ranking in recommender systems.",
      "judge_confidence": 0.9,
      "abstract": "Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate genera"
    },
    {
      "paper_id": "S2:b486982fa7c68a8a08df1111ba9607119419c488",
      "title": "A survey on large language models for recommendation",
      "year": 2023,
      "citation_count": 651,
      "discovered_from": "W4414691664",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in recommendation systems.",
      "judge_confidence": 0.9,
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning, prompt"
    },
    {
      "paper_id": "S2:0cfdd655100055f234fd23ebecd915504b8e00e3",
      "title": "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
      "year": 2023,
      "citation_count": 400,
      "discovered_from": "W4415102543",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in personalized recommendations.",
      "judge_confidence": 0.9,
      "abstract": "Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building convers"
    },
    {
      "paper_id": "S2:8f3b6a299098eb2e615e344b2f76a23dfca4d9ca",
      "title": "CoLLM: Integrating Collaborative Embeddings Into Large Language Models for Recommendation",
      "year": 2023,
      "citation_count": 143,
      "discovered_from": "W4415102543",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs for personalized recommendations, integrating collaborative information.",
      "judge_confidence": 0.9,
      "abstract": "Leveraging Large Language Models as recommenders, referred to as LLMRec, is gaining traction and brings novel dynamics for modeling user preferences, particularly for cold-start users. However, existing LLMRec approaches primarily focus on text semantics and overlook the crucial aspect of incorporating collaborative information from user-item interactions, leading to potentially sub-optimal performance in warm-start scenarios. To ensure superior recommendations across both warm and cold scenario"
    },
    {
      "paper_id": "S2:ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3",
      "title": "Is ChatGPT a Good Recommender? A Preliminary Study",
      "year": 2023,
      "citation_count": 348,
      "discovered_from": "W4414691664",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly explores ChatGPT for personalized recommendations.",
      "judge_confidence": 0.9,
      "abstract": "Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However, most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper, we employ ChatGPT "
    },
    {
      "paper_id": "S2:006aa1580fae5968417538c7acb4662c7b58088f",
      "title": "LLM-Rec: Personalized Recommendation via Prompting Large Language Models",
      "year": 2023,
      "citation_count": 124,
      "discovered_from": "W7092283700",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs for personalized recommendations.",
      "judge_confidence": 0.9,
      "abstract": "Text-based recommendation holds a wide range of practical applications due to its versatility, as textual descriptions can represent nearly any type of item. However, directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study, we int"
    },
    {
      "paper_id": "S2:450b5490cc653478c272be50aa986798df828a20",
      "title": "Uncovering ChatGPT’s Capabilities in Recommender Systems",
      "year": 2023,
      "citation_count": 322,
      "discovered_from": "W4407806925",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in recommender systems.",
      "judge_confidence": 0.9,
      "abstract": "The debut of ChatGPT has recently attracted significant attention from the natural language processing (NLP) community and beyond. Existing studies have demonstrated that ChatGPT shows significant improvement in a range of downstream NLP tasks, but the capabilities and limitations of ChatGPT in terms of recommendations remain unclear. In this study, we aim to enhance ChatGPT’s recommendation capabilities by aligning it with traditional information retrieval (IR) ranking capabilities, including p"
    },
    {
      "paper_id": "S2:4829b73a47be18f73e9e8d90f3c23c8f84d0fccb",
      "title": "Representation Learning with Large Language Models for Recommendation",
      "year": 2023,
      "citation_count": 309,
      "discovered_from": "W4407806925",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in recommendation systems.",
      "judge_confidence": 0.9,
      "abstract": "Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges f"
    },
    {
      "paper_id": "S2:0383e049e98c9eedbc61be728d4ef037300bbedf",
      "title": "Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach",
      "year": 2023,
      "citation_count": 301,
      "discovered_from": "W4403851269",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs for personalized recommendations.",
      "judge_confidence": 0.9,
      "abstract": "In the past few decades, recommender systems have attracted much attention in both research and industry communities. Existing recommendation models mainly learn the underlying user preference from historical behavior data (typically in the forms of item IDs), and then estimate the user–item matching relationships for recommendations. Inspired by the recent progress on large language models (LLMs), we develop a different recommendation paradigm, considering recommendation as instruction followin"
    },
    {
      "paper_id": "S2:b230b3c043b0671985bd56fcb0b23f3e9e29c762",
      "title": "HLLM: Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling",
      "year": 2024,
      "citation_count": 70,
      "discovered_from": "W4414691664",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in sequential recommendation systems.",
      "judge_confidence": 0.9,
      "abstract": "Large Language Models (LLMs) have achieved remarkable success in various fields, prompting several studies to explore their potential in recommendation systems. However, these attempts have so far resulted in only modest improvements over traditional recommendation models. Moreover, three critical questions remain under-explored: firstly, the real value of LLMs' pre-trained weights, often considered to encapsulate world knowledge; secondly, the necessity of fine-tuning for recommendation tasks; "
    },
    {
      "paper_id": "S2:46d382ef09f6da85e6d56b96bf48502990cf4fd2",
      "title": "LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential Recommendation",
      "year": 2024,
      "citation_count": 67,
      "discovered_from": "W4414691664",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in sequential recommendation systems.",
      "judge_confidence": 0.9,
      "abstract": "Sequential recommender systems (SRS) aim to predict users' subsequent choices based on their historical interactions and have found applications in diverse fields such as e-commerce and social media. However, in real-world systems, most users interact with only a handful of items, while the majority of items are seldom consumed. These two issues, known as the long-tail user and long-tail item challenges, often pose difficulties for existing SRS. These challenges can adversely affect user experie"
    },
    {
      "paper_id": "S2:690edf44e8739fd80bdfb76f40c9a4a222f3bba8",
      "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer",
      "year": 2019,
      "citation_count": 2758,
      "discovered_from": "W4403851269",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces BERT for sequential recommendations, key for LLM-based systems.",
      "judge_confidence": 0.9,
      "abstract": "Modeling users' dynamic preferences from their historical behaviors is challenging and crucial for recommendation systems. Previous methods employ sequential neural networks to encode users' historical interactions from left to right into hidden representations for making recommendations. Despite their effectiveness, we argue that such left-to-right unidirectional models are sub-optimal due to the limitations including: \\begin enumerate* [label=series\\itshape\\alph*\\upshape)] \\item unidirectional"
    },
    {
      "paper_id": "S2:79395f4ef32b925381f7ec9a824b05bdb982fd33",
      "title": "Large Language Models as Zero-Shot Conversational Recommenders",
      "year": 2023,
      "citation_count": 218,
      "discovered_from": "W4404313621",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in conversational recommendation tasks.",
      "judge_confidence": 0.9,
      "abstract": "In this paper, we present empirical studies on conversational recommendation tasks using representative large language models in a zero-shot setting with three primary contributions. (1) Data: To gain insights into model behavior in \"in-the-wild\" conversational recommendation scenarios, we construct a new dataset of recommendation-related conversations by scraping a popular discussion website. This is the largest public real-world conversational recommendation dataset to date. (2) Evaluation: On"
    },
    {
      "paper_id": "S2:009f64d890d8402cae8739b63bc0c3d06d62ed06",
      "title": "NoteLLM: A Retrievable Large Language Model for Note Recommendation",
      "year": 2024,
      "citation_count": 53,
      "discovered_from": "W4414691664",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs for personalized note recommendations.",
      "judge_confidence": 0.9,
      "abstract": "People enjoy sharing \"notes\" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings"
    },
    {
      "paper_id": "S2:f8df9546eafe207a777466ef40d548c707744c6d",
      "title": "LLMRG: Improving Recommendations through Large Language Model Reasoning Graphs",
      "year": 2024,
      "citation_count": 44,
      "discovered_from": "W7092283700",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs for personalized recommendations.",
      "judge_confidence": 0.9,
      "abstract": "Recommendation systems aim to provide users with relevant suggestions, but often lack interpretability and fail to capture higher-level semantic relationships between user behaviors and profiles. In this paper, we propose a novel approach that leverages large language models (LLMs) to construct personalized reasoning graphs. These graphs link a user's profile and behavioral sequences through causal and logical inferences, representing the user's interests in an interpretable way. Our approach, L"
    },
    {
      "paper_id": "S2:96b2902eba473d1be9e7b38b3c58c867bdca4f08",
      "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era",
      "year": 2024,
      "citation_count": 171,
      "discovered_from": "W6891779303",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Addresses bias in LLMs for recommender systems",
      "judge_confidence": 0.8,
      "abstract": "With the rapid advancements of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems "
    },
    {
      "paper_id": "S2:13965d8d68217308ff8c7e738ced637739c6a1b8",
      "title": "Bridging Language and Items for Retrieval and Recommendation",
      "year": 2024,
      "citation_count": 297,
      "discovered_from": "W7092283700",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces LLM-based model for item recommendation.",
      "judge_confidence": 0.9,
      "abstract": "This paper introduces BLaIR, a series of pretrained sentence embedding models specialized for recommendation scenarios. BLaIR is trained to learn correlations between item metadata and potential natural language context, which is useful for retrieving and recommending items. To pretrain BLaIR, we collect Amazon Reviews 2023, a new dataset comprising over 570 million reviews and 48 million items from 33 categories, significantly expanding beyond the scope of previous versions. We evaluate the gen"
    },
    {
      "paper_id": "S2:60f8a7ac53585aa2c173219e97507d6d963864e7",
      "title": "PALR: Personalization Aware LLMs for Recommendation",
      "year": 2023,
      "citation_count": 151,
      "discovered_from": "W4404343192",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs for personalized recommendations.",
      "judge_confidence": 0.9,
      "abstract": "Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user pre"
    },
    {
      "paper_id": "S2:26059f871eea2ef9aeeda228ebd40a69b61ab65c",
      "title": "RecMind: Large Language Model Powered Agent For Recommendation",
      "year": 2023,
      "citation_count": 151,
      "discovered_from": "W4404313621",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs for personalized recommendations.",
      "judge_confidence": 0.9,
      "abstract": "While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide ze"
    },
    {
      "paper_id": "S2:026b3396a63ed5772329708b7580d633bb86bec9",
      "title": "RWKV: Reinventing RNNs for the Transformer Era",
      "year": 2023,
      "citation_count": 859,
      "discovered_from": "W4386148472",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Foundational work on model architecture relevant to LLMs.",
      "judge_confidence": 0.7,
      "abstract": "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines t"
    },
    {
      "paper_id": "S2:fede400a1f7e75169987270d602e4e2f691818bd",
      "title": "Harnessing Large Language Models for Text-Rich Sequential Recommendation",
      "year": 2024,
      "citation_count": 94,
      "discovered_from": "W4414691664",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in recommender systems with novel techniques.",
      "judge_confidence": 0.9,
      "abstract": "Recent advances in Large Language Models (LLMs) have been changing the paradigm of Recommender Systems (RS). However, when items in the recommendation scenarios contain rich textual information, such as product descriptions in online shopping or news headlines on social media, LLMs require longer texts to comprehensively depict the historical user behavior sequence. This poses significant challenges to LLM-based recommenders, such as over-length limitations, extensive time and space overheads, a"
    },
    {
      "paper_id": "S2:429e6c09eeadf54e2b245b8f2cddfbf157f9da4c",
      "title": "ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation",
      "year": 2023,
      "citation_count": 136,
      "discovered_from": "W4404343192",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in recommendation tasks.",
      "judge_confidence": 0.9,
      "abstract": "With large language models (LLMs) achieving remarkable breakthroughs in NLP domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful informati"
    },
    {
      "paper_id": "S2:85722b13631d9846866d45ff2bfc2a2fe1026ac8",
      "title": "LLMRec: Benchmarking Large Language Models on Recommendation Task",
      "year": 2023,
      "citation_count": 49,
      "discovered_from": "W4415102543",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in recommendation tasks.",
      "judge_confidence": 0.9,
      "abstract": "Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM"
    },
    {
      "paper_id": "S2:aae1d88c70cf18ef6aa23693a4dce8204e22d087",
      "title": "A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems",
      "year": 2023,
      "citation_count": 133,
      "discovered_from": "W4404313621",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in recommendation systems.",
      "judge_confidence": 0.9,
      "abstract": "As the focus on Large Language Models (LLMs) in the field of recommendation intensifies, the optimization of LLMs for recommendation purposes (referred to as LLM4Rec) assumes a crucial role in enhancing their recommendation performance. However, existing approaches for LLM4Rec often assess performance using restricted sets of candidates, which may not accurately reflect the models’ overall ranking capabilities. In this article, our objective is to pursue LLM4Rec models with comprehensive ranking"
    },
    {
      "paper_id": "S2:c5481668f78ab0c8ef2de9230f2fc1ce27eea6e4",
      "title": "Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models",
      "year": 2023,
      "citation_count": 132,
      "discovered_from": "W4404313621",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs for enhancing recommendation systems.",
      "judge_confidence": 0.9,
      "abstract": "Recommender system plays a vital role in various online services. However, its insulated nature of training and deploying separately within a specific closed domain limits its access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capabilities. Nevertheless, previous attempts to directly use LLMs as recommenders cannot meet the inference latency demand of indu"
    },
    {
      "paper_id": "S2:95840ec08ba2e30490ee02fd0518bbfc0e8a0f4c",
      "title": "Prompt Engineering for Large Language Models",
      "year": 2023,
      "citation_count": 341,
      "discovered_from": "W4398795244",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Foundational work on LLMs, relevant for recommendation methods.",
      "judge_confidence": 0.7,
      "abstract": null
    },
    {
      "paper_id": "S2:b07493b1393f0b232872316f2b88fe0043b1ab02",
      "title": "Enhancing Sequential Recommendation via LLM-based Semantic Embedding Learning",
      "year": 2024,
      "citation_count": 73,
      "discovered_from": "W4414691664",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs in sequential recommendation systems.",
      "judge_confidence": 0.9,
      "abstract": "Sequential recommendation systems (SRS) are crucial in various applications as they enable users to discover relevant items based on their past interactions. Recent advancements involving large language models (LLMs) have shown significant promise in addressing intricate recommendation challenges. However, these efforts exhibit certain limitations. Specifically, directly extracting representations from an LLM based on items' textual features and feeding them into a sequential model hold no guara"
    },
    {
      "paper_id": "S2:ef0679f8b3114c339bdf5a0c202403a08d160a88",
      "title": "ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models",
      "year": 2023,
      "citation_count": 107,
      "discovered_from": "W4407632341",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses LLMs for content-based recommendations.",
      "judge_confidence": 0.9,
      "abstract": "Personalized content-based recommender systems have become indispensable tools for users to navigate through the vast amount of content available on platforms like daily news websites and book recommendation services. However, existing recommenders face significant challenges in understanding the content of items. Large language models (LLMs), which possess deep semantic comprehension and extensive knowledge from pretraining, have proven to be effective in various natural language processing tas"
    },
    {
      "paper_id": "S2:c589a3420ba335a05c248f525ea3c6e90215e42b",
      "title": "Pre-train, Prompt, and Recommendation: A Comprehensive Survey of Language Modeling Paradigm Adaptations in Recommender Systems",
      "year": 2023,
      "citation_count": 106,
      "discovered_from": "W4403851269",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Surveys LLM adaptations in recommender systems, directly relevant to core topic.",
      "judge_confidence": 0.9,
      "abstract": "Abstract The emergence of Pre-trained Language Models (PLMs) has achieved tremendous success in the field of Natural Language Processing (NLP) by learning universal representations on large corpora in a self-supervised manner. The pre-trained models and the learned representations can be beneficial to a series of downstream NLP tasks. This training paradigm has recently been adapted to the recommendation domain and is considered a promising approach by both academia and industry. In this paper, "
    },
    {
      "paper_id": "S2:c3a59e1e405e7c28319e5a1c5b5241f9b340cf63",
      "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
      "year": 2023,
      "citation_count": 278,
      "discovered_from": "W4407806925",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces a memory mechanism for LLMs, relevant for personalized recommendations.",
      "judge_confidence": 0.8,
      "abstract": "Large Language Models (LLMs) have drastically reshaped our interactions with artificial intelligence (AI) systems, showcasing impressive performance across an extensive array of tasks. Despite this, a notable hindrance remains—the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems, psychological counseling, and secretarial assistance. Recognizing the necessit"
    },
    {
      "paper_id": "S2:11628f656257e75e46447ac21cdaa86c4b340a0a",
      "title": "Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited",
      "year": 2023,
      "citation_count": 274,
      "discovered_from": "W4414691664",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Discusses modality-based models relevant to LLMs in recommendations.",
      "judge_confidence": 0.8,
      "abstract": "Recommendation models that utilize unique identities (IDs for short) to represent distinct users and items have been state-of-the-art (SOTA) and dominated the recommender systems (RS) literature for over a decade. Meanwhile, the pre-trained modality encoders, such as BERT [9] and Vision Transformer [11], have become increasingly powerful in modeling the raw modality features of an item, such as text and images. Given this, a natural question arises: can a purely modality-based recommendation mod"
    },
    {
      "paper_id": "S2:df602516e28a9ef0ef665ed0aef551984d8d770d",
      "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)",
      "year": 2022,
      "citation_count": 683,
      "discovered_from": "W4403851269",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces a unified LLM-based framework for personalized recommendations.",
      "judge_confidence": 0.9,
      "abstract": "For a long time, different recommendation tasks require designing task-specific architectures and training objectives. As a result, it is hard to transfer the knowledge and representations from one task to another, thus restricting the generalization ability of existing recommendation approaches. To deal with such issues, considering that language can describe almost anything and language grounding is a powerful medium to represent various problems or tasks, we present a flexible and unified tex"
    }
  ]
}