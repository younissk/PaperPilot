{
  "query": "Multi modal LLMs",
  "total_accepted": 41,
  "papers": [
    {
      "paper_id": "W4388555645",
      "title": "TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models",
      "abstract": "Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence w",
      "year": 2023,
      "citation_count": 0,
      "discovered_from": "evaluation metrics for multi-modal LLMs performance comparison",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0
    },
    {
      "paper_id": "W7123678370",
      "title": "FOCAL: A Novel Benchmarking Technique for Multi-modal Agents",
      "abstract": "With the recent advancements in reasoning capabilities, tool calling using MCP servers and Audio Language Models (ALMs), development and integration of multi-modal agents (with voice and text support) has come to the industry forefront. Cascading pipelines for voice agents still play a central role in the industry owing to their superior reasoning capabilities facilitated by LLMs. Although, cascading pipelines often present error propagation through the pipeline. We propose a framework, FOCAL to",
      "year": 2026,
      "citation_count": 0,
      "discovered_from": "evaluation metrics for multi-modal LLMs performance comparison",
      "edge_type": "seed",
      "depth": 0,
      "judge_reason": "Seed paper from initial search",
      "judge_confidence": 1.0
    },
    {
      "paper_id": "S2:fa75a55760e6ea49b39b83cb85c99a22e1088254",
      "title": "NExT-GPT: Any-to-Any Multimodal LLM",
      "abstract": "While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end ge",
      "year": 2023,
      "citation_count": 713,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses multi-modal LLM capabilities and integration.",
      "judge_confidence": 0.95
    },
    {
      "paper_id": "S2:5cac6430bd379c9d2fe13137dfd6ae7721a2679f",
      "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
      "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech repres",
      "year": 2023,
      "citation_count": 527,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses multi-modal LLMs with cross-modal capabilities.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:43e6e8d6663d83f1b74cf5a2be7b040b0928f867",
      "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
      "abstract": "Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into for",
      "year": 2023,
      "citation_count": 150,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses multi-modal LLM integration and capabilities.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:ebedc4d7a2356090904baba4104ef0832bc236df",
      "title": "A survey on multimodal large language models",
      "abstract": "ABSTRACT Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of the MLLM, such as writing stories based on images and optical character recognitionâ€“free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and ind",
      "year": 2023,
      "citation_count": 1005,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses multimodal large language models and their applications.",
      "judge_confidence": 0.95
    },
    {
      "paper_id": "S2:a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5",
      "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT",
      "abstract": "Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through ",
      "year": 2023,
      "citation_count": 727,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Discusses multimodal interaction and generative models relevant to multi-modal LLMs.",
      "judge_confidence": 0.85
    },
    {
      "paper_id": "S2:780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050",
      "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
      "abstract": "Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can ",
      "year": 2023,
      "citation_count": 713,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses multimodal integration in LLMs.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representat",
      "year": 2023,
      "citation_count": 6706,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Core paper on multi-modal LLMs for vision-language tasks.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",
      "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
      "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated A",
      "year": 2023,
      "citation_count": 1232,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Addresses multi-modal tasks using LLMs, relevant to core topic.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:3efb81de24eb88017d6dbcf22cb4215084223fd8",
      "title": "AudioPaLM: A Large Language Model That Can Speak and Listen",
      "abstract": "We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from Audio",
      "year": 2023,
      "citation_count": 397,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Core paper on multi-modal LLM integrating text and speech.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:cb8dcaf8e5fe7256577c6bc83e11dd64d8f3ae31",
      "title": "Towards artificial general intelligence via a multimodal foundation model",
      "abstract": "The fundamental goal of artificial intelligence (AI) is to mimic the core cognitive activities of human. Despite tremendous success in the AI research, most of existing methods have only single-cognitive ability. To overcome this limitation and take a solid step towards artificial general intelligence (AGI), we develop a foundation model pre-trained with huge multimodal data, which can be quickly adapted for various downstream cognitive tasks. To achieve this goal, we propose to pre-train our fo",
      "year": 2021,
      "citation_count": 288,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Discusses a multimodal foundation model for various cognitive tasks.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:8bc617c9139648d7a92991d70c671230bac7b2e2",
      "title": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",
      "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numer",
      "year": 2023,
      "citation_count": 334,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Addresses multi-modal LLMs with audio integration.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:d3135733aa39dec20ce72aa138589dda27c8406d",
      "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
      "abstract": "When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the t",
      "year": 2022,
      "citation_count": 1874,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Addresses multimodal reasoning in LLMs for question answering.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:8b5eab31e1c5689312fff3181a75bfbf5c13e51c",
      "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",
      "abstract": "We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs perta",
      "year": 2022,
      "citation_count": 475,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Core paper on multi-modal LLMs integrating vision and language tasks.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:376f494126d1ea4f571ea0263c43ac2b6331800a",
      "title": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs",
      "abstract": "In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehe",
      "year": 2023,
      "citation_count": 69,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses multimodal generation with LLMs.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:ebe259796870ebccf26577044d0087884209b884",
      "title": "w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
      "abstract": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked ",
      "year": 2021,
      "citation_count": 501,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces methods for speech representation relevant to multi-modal LLMs.",
      "judge_confidence": 0.8
    },
    {
      "paper_id": "S2:4fffa5245d3972077c83614c2a08a47cb578631e",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provid",
      "year": 2021,
      "citation_count": 4040,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Foundational work on speech representation relevant to multi-modal LLMs.",
      "judge_confidence": 0.8
    },
    {
      "paper_id": "S2:985f0c89c5a607742ec43c1fdc2cbfe54541cbad",
      "title": "Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation",
      "abstract": "While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token v",
      "year": 2023,
      "citation_count": 524,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces a tokenizer for visual generation with LLMs, relevant to multi-modal integration.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:124d4d374fbef2016fa9880489871a58a7450644",
      "title": "Improved Baselines with Visual Instruction Tuning",
      "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data wi",
      "year": 2023,
      "citation_count": 4207,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Core paper on multi-modal LLMs with significant contributions.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:4cc8e18f5eece0b0d8e1abcb8ee10fb33680fbb2",
      "title": "Simple and Controllable Music Generation",
      "abstract": "We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samp",
      "year": 2023,
      "citation_count": 593,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Involves multi-modal generation with text and music representation.",
      "judge_confidence": 0.8
    },
    {
      "paper_id": "S2:5d321194696f1f75cf9da045e6022b2f20ba5b9c",
      "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
      "abstract": "We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes",
      "year": 2023,
      "citation_count": 1495,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses multi-modal LLMs for video understanding.",
      "judge_confidence": 0.95
    },
    {
      "paper_id": "S2:570079bbdd8758dfe865097e05719313c9c1301a",
      "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
      "abstract": "How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augme",
      "year": 2023,
      "citation_count": 707,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Core paper on multi-modal LLMs with visual instruction integration.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:a5036f31f0e629dc661f120b8c3b1f374d479ab8",
      "title": "Visual Instruction Tuning",
      "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connec",
      "year": 2023,
      "citation_count": 7496,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Core paper on multimodal LLMs and instruction tuning.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:a757999ed260d7bc45484dc6b4456bf33fe6f679",
      "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
      "abstract": "We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating",
      "year": 2023,
      "citation_count": 940,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces methods for multi-modal instruction learning with LLMs.",
      "judge_confidence": 0.85
    },
    {
      "paper_id": "S2:2c686c6c978263fdcd1f47b558d4570ae272ac3e",
      "title": "SeamlessM4T: Massively Multilingual&Multimodal Machine Translation",
      "abstract": "What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages? While recent breakthroughs in text-based models have pushed machine translation coverage beyond 200 languages, unified speech-to-speech translation models have yet to achieve similar strides. More specifically, conventional speech-to-speech translation systems rely on cascaded systems that perform translation progressively, putting high-performing unified systems out of reach.",
      "year": 2023,
      "citation_count": 144,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Addresses multi-modal translation using LLMs for speech and text.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:40298b8d50109c52fc10763eddc64a07cf8acb31",
      "title": "Planting a SEED of Vision in Large Language Model",
      "abstract": "We present SEED, an elaborate image tokenizer that empowers Large Language Models (LLMs) with the emergent ability to SEE and Draw at the same time. Research on image tokenizers has previously reached an impasse, as frameworks employing quantized visual tokens have lost prominence due to subpar performance and convergence in multimodal comprehension (compared to BLIP-2, etc.) or generation (compared to Stable Diffusion, etc.). Despite the limitations, we remain confident in its natural capacity ",
      "year": 2023,
      "citation_count": 126,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces methods for integrating image and text in LLMs.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:47f7ec3d0a5e6e83b6768ece35206a94dc81919c",
      "title": "Taming Transformers for High-Resolution Image Synthesis",
      "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby s",
      "year": 2020,
      "citation_count": 3800,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces transformer methods for image synthesis, relevant to multi-modal applications.",
      "judge_confidence": 0.8
    },
    {
      "paper_id": "S2:67dea28495cab71703993d0d52ca4733b9a66077",
      "title": "Jukebox: A Generative Model for Music",
      "abstract": "We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more control",
      "year": 2020,
      "citation_count": 900,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Generates music with singing, integrating audio and text modalities.",
      "judge_confidence": 0.8
    },
    {
      "paper_id": "S2:e7d09b6f2bc878cf2c993acf675f409d0b55f35a",
      "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
      "abstract": "The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training stra",
      "year": 2023,
      "citation_count": 119,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Directly addresses multimodal LLMs and their generation capabilities.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:ffa05cb5504ba08254f498223f613b3ebcf87692",
      "title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT",
      "abstract": "Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using co",
      "year": 2023,
      "citation_count": 102,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Addresses multi-modal LLMs for audio and text tasks.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks t",
      "year": 2022,
      "citation_count": 4888,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Core paper on multi-modal LLMs with innovative architecture for visual and textual data.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:f4793adffd6f67ffcb93ccfc5672ab301b8a2b96",
      "title": "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering",
      "abstract": "Medical Visual Question Answering (MedVQA) presents a significant opportunity to enhance diagnostic accuracy and healthcare delivery by leveraging artificial intelligence to interpret and answer questions based on medical images. In this study, we reframe the problem of MedVQA as a generation task that naturally follows the human-machine interaction and propose a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large ",
      "year": 2023,
      "citation_count": 269,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Addresses multi-modal LLMs for medical visual question answering.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:599be9043ef3571f65758cf36e184c9dc1781baf",
      "title": "BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers",
      "abstract": "Masked image modeling (MIM) has demonstrated impressive results in selfsupervised representation learning by recovering corrupted image patches. However, most existing studies operate on low-level image pixels, which hinders the exploitation of high-level semantics for representation models. In this work, we propose to use a semantic-rich visual tokenizer as the reconstruction target for masked prediction, providing a systematic way to promote MIM from pixel-level to semantic-level. Specifically",
      "year": 2022,
      "citation_count": 394,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Foundational work on visual tokenization for multi-modal tasks.",
      "judge_confidence": 0.8
    },
    {
      "paper_id": "S2:9c7a2cd13b783bb73ad2d1ec2880bdd9b995cbdc",
      "title": "Vector-quantized Image Modeling with Improved VQGAN",
      "abstract": "Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling (VIM) approach that involves pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformer-based VQGAN (ViT-VQG",
      "year": 2021,
      "citation_count": 682,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces methods for image modeling relevant to multi-modal LLMs.",
      "judge_confidence": 0.8
    },
    {
      "paper_id": "S2:2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
      "title": "Zero-Shot Text-to-Image Generation",
      "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive",
      "year": 2021,
      "citation_count": 6011,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Addresses multi-modal generation using transformers, relevant to LLMs.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:65b20b3e6f81f6567e41e353a47a2380c6cc4b77",
      "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling",
      "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) (CITATION) is the only prior work addressing the generative aspect of speech pre-training, which builds a text-free language model using discovered units. Unfortunately, because the units used in GSLM discard most prosodic information, GSLM fails",
      "year": 2021,
      "citation_count": 139,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces a multi-modal generative model for speech, relevant to LLMs.",
      "judge_confidence": 0.8
    },
    {
      "paper_id": "S2:f8fd0fa676f1d1d1f9d28dd81910cfb92ef7ac86",
      "title": "Adapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition",
      "abstract": "Most end-to-end (E2E) speech recognition models are composed of encoder and decoder blocks that perform acoustic and language modeling functions. Pretrained large language models (LLMs) have the potential to improve the performance of E2E ASR. However, integrating a pretrained language model into an E2E speech recognition model has shown limited benefits due to the mismatches between text-based LLMs and those used in E2E ASR. In this paper, we explore an alternative approach by adapting a pretra",
      "year": 2023,
      "citation_count": 20,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Integrates LLMs with speech, aligning with multi-modal applications.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:0fe88452660cb8a0e37f54bcd44f3cd6504354b5",
      "title": "Unified Model for Image, Video, Audio and Language Tasks",
      "abstract": "Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mi",
      "year": 2023,
      "citation_count": 54,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Addresses multi-modal LLMs integrating text, images, video, and audio.",
      "judge_confidence": 0.9
    },
    {
      "paper_id": "S2:f466157848d1a7772fb6d02cdac9a7a5e7ef982e",
      "title": "Neural Discrete Representation Learning",
      "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vec",
      "year": 2017,
      "citation_count": 6415,
      "discovered_from": "W4388555645",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Introduces generative model for multi-modal representation learning.",
      "judge_confidence": 0.8
    },
    {
      "paper_id": "S2:390342245563699a88bd5b8f4093e2c0e199b0c2",
      "title": "Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface",
      "abstract": "Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle. This is what prevents employment of Language models in knowledge retrieval automation. This becomes accentuated when one wants to integrate a speech interface on top of a text based knowledge retrieval system. Besides, for commercial search and chat-bot applications, complete reliance on commercial large language models (LLMs) li",
      "year": 2023,
      "citation_count": 19,
      "discovered_from": "W7123678370",
      "edge_type": "reference",
      "depth": 1,
      "judge_reason": "Integrates speech interface with LLMs for multi-modal information retrieval.",
      "judge_confidence": 0.8
    }
  ]
}